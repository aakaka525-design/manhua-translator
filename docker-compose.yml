services:
  api:
    platform: linux/amd64
    build:
      context: .
      dockerfile: docker/Dockerfile.api
      args:
        INSTALL_LAMA: "${INSTALL_LAMA:-0}"
    env_file:
      - .env
    environment:
      AUTO_SETUP_MODELS: "${AUTO_SETUP_MODELS:-on}"
      MODEL_WARMUP_TIMEOUT: "${MODEL_WARMUP_TIMEOUT:-300}"
      OCR_WARMUP_LANGS: "${OCR_WARMUP_LANGS:-korean}"
      LAMA_DEVICE: "${LAMA_DEVICE:-cpu}"
      FLAGS_use_mkldnn: "${FLAGS_use_mkldnn:-0}"
      FLAGS_use_pir_api: "${FLAGS_use_pir_api:-0}"
      PADDLE_PDX_DISABLE_MODEL_SOURCE_CHECK: "${PADDLE_PDX_DISABLE_MODEL_SOURCE_CHECK:-True}"
      DEBUG_OCR: "${DEBUG_OCR:-0}"
      DEBUG_TRANSLATOR: "${DEBUG_TRANSLATOR:-0}"
      DEBUG_WATERMARK: "${DEBUG_WATERMARK:-0}"
      DEBUG_ARTIFACTS: "${DEBUG_ARTIFACTS:-0}"
    volumes:
      - ./data:/app/data
      - ./output:/app/output
      - ./logs:/app/logs
      - ./models:/root/.paddlex
    ports:
      - "8000:8000"
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/api/v1/system/models', timeout=5).read()"]
      start_period: 60s
      interval: 10s
      timeout: 5s
      retries: 24

  web:
    platform: linux/amd64
    build:
      context: .
      dockerfile: docker/Dockerfile.web
    depends_on:
      api:
        condition: service_healthy
    ports:
      - "80:80"
